{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmDKr0REkQ9W",
        "outputId": "67503769-4563-48b7-95bd-702a7e4f8baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/My Drive/sign_language\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ri6rMCgTkbpt",
        "outputId": "4f6ae1ad-2f19-4f5f-c70b-bf78c2ddaa0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/sign_language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kimjaebeom98/Sign-Language-Translator.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYtLzAgtk3vb",
        "outputId": "9349a367-9112-4e4e-dc92-46dfd05fb827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sign-Language-Translator'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 31 (delta 4), reused 28 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (31/31), 10.64 MiB | 14.56 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqBpLdnuqwpr",
        "outputId": "4315982a-e1ff-4271-d5a3-f82314154cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.5.26)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.23.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.8.0.76)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.5 sounddevice-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import base64,cv2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "import time, io, os, time, sys, natsort, random, math"
      ],
      "metadata": {
        "id": "HW4HyGYwk8nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp_holistic = mp.solutions.holistic\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "def mediapipe_detection(image, model):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image.flags.writeable = False\n",
        "    results = model.process(image)\n",
        "    image.flags.writeable = True\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    return image, results\n",
        "\n",
        "# 왼손, 오른손 key_point 추출\n",
        "def extract_keypoints(results):\n",
        "    lh = np.array([[res.x*3, res.y*3, res.z*3] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "    rh = np.array([[res.x*3, res.y*3, res.z*3] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "    return np.concatenate([lh, rh])\n",
        "\n",
        "my_dict ={\"None\":0, \"계산\":1, \"고맙다\":2, \"괜찮다\":3, \"기다리다\":4, \"나\" :5, \"네\": 6,\n",
        "          \"다음\":7, \"달다\":8, \"더\":9, \"도착\":10, \"돈\":11, \"또\":12,\n",
        "          \"맵다\":13, \"먼저\":14, \"무엇\":15, \"물\":16, \"물음\":17, \"부탁\":18, \"사람\":19,\n",
        "          \"수저\":20, \"시간\":21, \"아니요\":22, \"어디\":23, \"얼마\":24,\"예약\":25,\n",
        "          \"오다\":26, \"우리\":27, \"음식\":28, \"이거\":29, \"인기\":30, \"있다\":31, \"자리\":32,\n",
        "          \"접시\":33, \"제일\":34, \"조금\":35, \"주문\":36, \"주세요\":37, \"짜다\":38, \"책\":39,\n",
        "          \"추천\":40, \"화장실\":41, \"확인\":42}\n",
        "\n",
        "## 받은 5개의 단어들을 데이터 프레임으로 변환\n",
        "def make_word_df(word0, word1, word2, word3, word4):\n",
        "    info = [[word0, word1, word2, word3, word4]]\n",
        "    df = pd.DataFrame(info, columns = ['target0', 'target1', 'target2', 'target3', 'target4'])\n",
        "    return df\n",
        "\n",
        "## 받은 단어를 숫자로 반환\n",
        "def get_key(val):\n",
        "    for key, value in my_dict.items():\n",
        "         if val == key:\n",
        "             return value\n",
        "\n",
        "    return \"There is no such Key\"\n",
        "\n",
        "## 인자로 받은 단어 5개의 데이터프레임을\n",
        "def make_num_df(input_1):\n",
        "    num_oflist = []\n",
        "    for i in input_1.columns:\n",
        "        num_oflist.append(get_key(input_1[i].values))\n",
        "    input2 = make_word_df(num_oflist[0], num_oflist[1], num_oflist[2], num_oflist[3], num_oflist[4])\n",
        "    return input2\n",
        "\n",
        "log_dir = os.path.join('Logs')\n",
        "tb_callback = TensorBoard(log_dir = log_dir)\n",
        "\n",
        "# Actions that we try to detect\n",
        "actions = np.array(['None', '계산', '고맙다', '괜찮다', '기다리다', '나', '네', '다음',\n",
        "                   '달다', '더', '도착', '돈', '또', '맵다', '먼저', '무엇', '물', '물음',\n",
        "                   '부탁', '사람', '수저', '시간', '아니요', '어디', '얼마', '예약', '오다',\n",
        "                   '우리', '음식', '이거', '인기', '있다', '자리', '접시', '제일', '조금',\n",
        "                   '주문', '주세요', '짜다', '책', '추천', '화장실', '확인'])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30, 126)))\n",
        "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
        "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(actions.shape[0], activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='Adam', loss ='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.load_weights(\"/content/drive/MyDrive/sign_language/Sign-Language-Translator/actionxhand_data0524_0513.h5\")\n",
        "rlf = joblib.load(\"/content/drive/MyDrive/sign_language/Sign-Language-Translator/sentence_model.pkl\")\n",
        "data = pd.read_excel(\"/content/drive/MyDrive/sign_language/Sign-Language-Translator/sentence_data.xlsx\", engine = 'openpyxl')\n",
        "data_x = data.drop(['sentence'], axis = 1)\n",
        "data_y = data['sentence']\n",
        "le = LabelEncoder()\n",
        "le.fit(data['sentence'])\n",
        "\n",
        "# font = ImageFont.truetype(\"fonts/HMFMMUEX.TTC\", 10)\n",
        "# font2 = ImageFont.truetype(\"fonts/HMFMMUEX.TTC\", 20)\n",
        "# blue_color = (255,0,0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "-b5Py-PflRJm",
        "outputId": "726b624d-1373-42b4-ec66-33e961d9a710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.24.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sequence = []\n",
        "sentence = []\n",
        "predictions = []\n",
        "count = 0\n",
        "sequence_length = 30\n",
        "filePath='/content/KakaoTalk_20231004_215652292.mp4'\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture(filePath)\n",
        "\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "  count = count+1\n",
        "\n",
        "\n",
        "  #ret, frame = cap.read()\n",
        "  # 프레임 카운터 초기화\n",
        "  frame_count = 0\n",
        "\n",
        "  # 추출할 프레임 간격 설정\n",
        "  frame_interval = 2  # 5프레임당 1장 추출\n",
        "  while True:\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # 비디오에서 프레임을 읽을 수 없으면 종료\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "\n",
        "    frame_count += 1\n",
        "    # frame_interval 만큼의 간격으로 프레임 저장\n",
        "    if frame_count % frame_interval == 0:\n",
        "      image, results = mediapipe_detection(frame, holistic)\n",
        "      keypoints = extract_keypoints(results)\n",
        "      sequence.append(keypoints)\n",
        "      # for frame_num in range(sequence_length):\n",
        "      #   image, results = mediapipe_detection(frame, holistic)\n",
        "      #   keypoints = extract_keypoints(results)\n",
        "      #   sequence.append(keypoints)\n",
        "\n",
        "    # 포즈 주석을 이미지 위에 그립니다.\n",
        "    image.flags.writeable = True\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    mp_drawing.draw_landmarks(\n",
        "        image,\n",
        "        results.pose_landmarks,\n",
        "        mp_pose.POSE_CONNECTIONS,\n",
        "        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
        "\n",
        "\n",
        "  print(len(sequence))\n",
        "  res = model.predict(np.expand_dims(sequence[:30], axis=0))[0]\n",
        "  print(actions[np.argmax(res)])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZojRWPcp_b4",
        "outputId": "7bb2bb62-9172-48a2-fb6b-031cad58c173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "물\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Frame count:', int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDNS6Z6UjfFm",
        "outputId": "812589d0-9ddf-42bd-d85c-bf6de39e1312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame count: 66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frame = (readb64(data_image))\n",
        "        with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "            count = count+1\n",
        "            # Make detections\n",
        "            image, results = mediapipe_detection(frame, holistic)\n",
        "            keypoints = extract_keypoints(results)\n",
        "            sequence.append(keypoints)\n",
        "            print(len(sequence))\n",
        "            if (len(sequence) % 30 == 0):\n",
        "                sentence_len1 = len(sentence)\n",
        "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
        "                print(actions[np.argmax(res)])\n",
        "\n",
        "                \"\"\"\n",
        "                predictions.append(np.argmax(res))\n",
        "                u = np.bincount(predictions[-1:])\n",
        "                b = u.argmax()\n",
        "                if b == np.argmax(res):\n",
        "\n",
        "                \"\"\"\n",
        "\n",
        "                if res[np.argmax(res)] > threshold:\n",
        "                    if len(sentence) > 0:\n",
        "                        if actions[np.argmax(res)] == 'None':\n",
        "                                sentence.append(actions[np.argmax(res)])\n",
        "                        else:\n",
        "                            if(actions[np.argmax(res)] != sentence[-1]):\n",
        "                                sentence.append(actions[np.argmax(res)])\n",
        "                    else:\n",
        "                        sentence.append(actions[np.argmax(res)])\n",
        "\n",
        "\n",
        "                sentence_len2 = len(sentence)\n",
        "                count = 0\n",
        "                sequence.clear()\n",
        "                if(sentence_len1 != sentence_len2):\n",
        "\n",
        "                    if(len(sentence) == 5):\n",
        "                        data_form = make_word_df(sentence[0], sentence[1], sentence[2], sentence[3], sentence[4])\n",
        "                        input_data = make_num_df(data_form)\n",
        "                        y_pred = rlf.predict(input_data)\n",
        "                        le.inverse_transform(y_pred)\n",
        "                        predict_word = np.array2string(le.inverse_transform(y_pred))\n",
        "                        sentence.clear()\n",
        "                        emit('result', predict_word)\n",
        "                    else:\n",
        "                        predict_word = sentence[-1]\n",
        "                        emit('response_back', predict_word)\n",
        "                else:\n",
        "                    predict_word = \"failed\"\n",
        "                    emit('response_back', predict_word)\n",
        "            emit('start', 'start')"
      ],
      "metadata": {
        "id": "t9qdZHzwmWRD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}